{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from argparse import Namespace\n",
    "config = Namespace(\n",
    "    seq_len = 128,\n",
    "    d_model = 32,\n",
    "    d_ffn = 32*4,\n",
    "    n_heads = 8,\n",
    "    n_layers = 8,\n",
    "    n_input_values = 4,\n",
    "    n_output_values = 4,\n",
    "    d_value = 4,\n",
    "    d_id = 32,\n",
    "    dtype = torch.bfloat16,\n",
    "    dropout = 0.1,\n",
    "    lr = 1e-3,\n",
    "    weight_decay = 1e-4,\n",
    "\n",
    "    # longformer\n",
    "    attention_window = 16,\n",
    "    attention_dilation = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theModel(\n",
      "  (embed): embedding_int(\n",
      "    (embed): Embedding(4, 4)\n",
      "  )\n",
      "  (id_embed): embedding_int(\n",
      "    (embed): Embedding(128, 32)\n",
      "  )\n",
      "  (correct_dim): Linear(in_features=36, out_features=32, bias=True)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x Block(\n",
      "      (mha): MHA(\n",
      "        (attn): LongformerSelfAttention(\n",
      "          (query): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (key): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (value): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (query_global): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (key_global): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (value_global): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (ffn): MLP(\n",
      "        (w1): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (w2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (w3): Linear(in_features=32, out_features=128, bias=True)\n",
      "      )\n",
      "      (attn_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (head): Linear(in_features=32, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# MODEL\n",
    "from model.longformer import theModel\n",
    "model = theModel(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/augix/miniconda3/envs/snp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_wrapper(\n",
      "  (model): theModel(\n",
      "    (embed): embedding_int(\n",
      "      (embed): Embedding(4, 4)\n",
      "    )\n",
      "    (id_embed): embedding_int(\n",
      "      (embed): Embedding(128, 32)\n",
      "    )\n",
      "    (correct_dim): Linear(in_features=36, out_features=32, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x Block(\n",
      "        (mha): MHA(\n",
      "          (attn): LongformerSelfAttention(\n",
      "            (query): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (key): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (value): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (query_global): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (key_global): Linear(in_features=32, out_features=32, bias=True)\n",
      "            (value_global): Linear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ffn): MLP(\n",
      "          (w1): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (w2): Linear(in_features=128, out_features=32, bias=True)\n",
      "          (w3): Linear(in_features=32, out_features=128, bias=True)\n",
      "        )\n",
      "        (attn_norm): RMSNorm()\n",
      "        (ffn_norm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "    (head): Linear(in_features=32, out_features=4, bias=True)\n",
      "  )\n",
      "  (train_acc): MulticlassAccuracy()\n",
      "  (val_acc): MulticlassAccuracy()\n",
      "  (get_confusion_matrix): MulticlassConfusionMatrix()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# wrap with pytorch lightning\n",
    "from trainer.pl_module import model_wrapper\n",
    "pl_model = model_wrapper(config, model) \n",
    "print(pl_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
